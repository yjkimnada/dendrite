{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c17b9b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tnrange\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "from scipy.stats import rankdata\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb47951",
   "metadata": {},
   "source": [
    "# Weighted Corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8968d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedCorr:\n",
    "    def __init__(self, xyw=None, x=None, y=None, w=None, df=None, wcol=None):\n",
    "        ''' Weighted Correlation class. Either supply xyw, (x, y, w), or (df, wcol). Call the class to get the result, i.e.:\n",
    "        WeightedCorr(xyw=mydata[[x, y, w]])(method='pearson')\n",
    "        :param xyw: pd.DataFrame with shape(n, 3) containing x, y, and w columns (column names irrelevant)\n",
    "        :param x: pd.Series (n, ) containing values for x\n",
    "        :param y: pd.Series (n, ) containing values for y\n",
    "        :param w: pd.Series (n, ) containing weights\n",
    "        :param df: pd.Dataframe (n, m+1) containing m phenotypes and a weight column\n",
    "        :param wcol: str column of the weight column in the dataframe passed to the df argument.\n",
    "        '''\n",
    "        if (df is None) and (wcol is None):\n",
    "            if np.all([i is None for i in [xyw, x, y, w]]):\n",
    "                raise ValueError('No data supplied')\n",
    "            if not ((isinstance(xyw, pd.DataFrame)) != (np.all([isinstance(i, pd.Series) for i in [x, y, w]]))):\n",
    "                raise TypeError('xyw should be a pd.DataFrame, or x, y, w should be pd.Series')\n",
    "            xyw = pd.concat([x, y, w], axis=1).dropna() if xyw is None else xyw.dropna()\n",
    "            self.x, self.y, self.w = (pd.to_numeric(xyw[i], errors='coerce').values for i in xyw.columns)\n",
    "            self.df = None\n",
    "        elif (wcol is not None) and (df is not None):\n",
    "            if (not isinstance(df, pd.DataFrame)) or (not isinstance(wcol, str)):\n",
    "                raise ValueError('df should be a pd.DataFrame and wcol should be a string')\n",
    "            if wcol not in df.columns:\n",
    "                raise KeyError('wcol not found in column names of df')\n",
    "            self.df = df.loc[:, [x for x in df.columns if x != wcol]]\n",
    "            self.w = pd.to_numeric(df.loc[:, wcol], errors='coerce')\n",
    "        else:\n",
    "            raise ValueError('Incorrect arguments specified, please specify xyw, or (x, y, w) or (df, wcol)')\n",
    "\n",
    "    def _wcov(self, x, y, ms):\n",
    "        return np.sum(self.w * (x - ms[0]) * (y - ms[1]))\n",
    "\n",
    "    def _pearson(self, x=None, y=None):\n",
    "        x, y = (self.x, self.y) if ((x is None) and (y is None)) else (x, y)\n",
    "        mx, my = (np.sum(i * self.w) / np.sum(self.w) for i in [x, y])\n",
    "        return self._wcov(x, y, [mx, my]) / np.sqrt(self._wcov(x, x, [mx, mx]) * self._wcov(y, y, [my, my]))\n",
    "\n",
    "    def _wrank(self, x):\n",
    "        (unique, arr_inv, counts) = np.unique(rankdata(x), return_counts=True, return_inverse=True)\n",
    "        a = np.bincount(arr_inv, self.w)\n",
    "        return (np.cumsum(a) - a)[arr_inv]+((counts + 1)/2 * (a/counts))[arr_inv]\n",
    "\n",
    "    def _spearman(self, x=None, y=None):\n",
    "        x, y = (self.x, self.y) if ((x is None) and (y is None)) else (x, y)\n",
    "        return self._pearson(self._wrank(x), self._wrank(y))\n",
    "\n",
    "    def __call__(self, method='pearson'):\n",
    "        '''\n",
    "        :param method: Correlation method to be used: 'pearson' for pearson r, 'spearman' for spearman rank-order correlation.\n",
    "        :return: if xyw, or (x, y, w) were passed to __init__ returns the correlation value (float).\n",
    "                 if (df, wcol) were passed to __init__ returns a pd.DataFrame (m, m), the correlation matrix.\n",
    "        '''\n",
    "        if method not in ['pearson', 'spearman']:\n",
    "            raise ValueError('method should be one of [\\'pearson\\', \\'spearman\\']')\n",
    "        cor = {'pearson': self._pearson, 'spearman': self._spearman}[method]\n",
    "        if self.df is None:\n",
    "            return cor()\n",
    "        else:\n",
    "            out = pd.DataFrame(np.nan, index=self.df.columns, columns=self.df.columns)\n",
    "            for i, x in enumerate(self.df.columns):\n",
    "                for j, y in enumerate(self.df.columns):\n",
    "                    if i >= j:\n",
    "                        out.loc[x, y] = cor(x=pd.to_numeric(self.df[x], errors='coerce'), y=pd.to_numeric(self.df[y], errors='coerce'))\n",
    "                        out.loc[y, x] = out.loc[x, y]\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0584e926",
   "metadata": {},
   "source": [
    "# RSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "170327e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29253/2177110709.py:24: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for i in tnrange(rep_no):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6f3fbc75d34e2bbe645d24effa3890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rep_no = 5\n",
    "\n",
    "C4_diff_all = np.zeros((rep_no, 2000,2000))\n",
    "C4_nona_all = np.zeros((rep_no, 2000,2000))\n",
    "C8_diff_all = np.zeros((rep_no, 2000,2000))\n",
    "C8_nona_all = np.zeros((rep_no, 2000,2000))\n",
    "C12_diff_all = np.zeros((rep_no, 2000,2000))\n",
    "C12_nona_all = np.zeros((rep_no, 2000,2000))\n",
    "\n",
    "C4_diff_scale_all = np.zeros((rep_no, 2000))\n",
    "C4_nona_scale_all = np.zeros((rep_no, 2000))\n",
    "C8_diff_scale_all = np.zeros((rep_no, 2000))\n",
    "C8_nona_scale_all = np.zeros((rep_no, 2000))\n",
    "C12_diff_scale_all = np.zeros((rep_no, 2000))\n",
    "C12_nona_scale_all = np.zeros((rep_no, 2000))\n",
    "\n",
    "C4_rand_all = np.zeros((rep_no, 2000, 2000))\n",
    "C4_rand_scale_all = np.random.rand(rep_no, 2000)\n",
    "C8_rand_all = np.zeros((rep_no, 2000, 2000))\n",
    "C8_rand_scale_all = np.random.rand(rep_no, 2000)\n",
    "C12_rand_all = np.zeros((rep_no, 2000, 2000))\n",
    "C12_rand_scale_all = np.random.rand(rep_no, 2000)\n",
    "\n",
    "for i in tnrange(rep_no):\n",
    "    if i == 0:\n",
    "        C4_diff_raw = np.load(\"/media/hdd01/sklee/CA1_clust4-60/clust/gru_s5_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"C_syn_e\"]\n",
    "        C4_nona_raw = np.load(\"/media/hdd01/sklee/CA1_clust4-60_noNA/clust/gru_s5_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"C_syn_e\"]\n",
    "        C8_diff_raw = np.load(\"/media/hdd01/sklee/CA1_clust8-30/clust/gru_s9_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"C_syn_e\"]\n",
    "        C8_nona_raw = np.load(\"/media/hdd01/sklee/CA1_clust8-30_noNA/clust/gru_s9_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"C_syn_e\"]\n",
    "        C12_diff_raw = np.load(\"/media/hdd01/sklee/CA1_clust12-20/clust/gru_s13_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"C_syn_e\"]\n",
    "        C12_nona_raw = np.load(\"/media/hdd01/sklee/CA1_clust12-20_noNA/clust/gru_s13_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"C_syn_e\"]\n",
    "\n",
    "        C4_dist_raw = np.load(\"/media/hdd01/sklee/CA1_clust4-60/data/clust4_syn_dist.npy\")[:,-1]\n",
    "        C8_dist_raw = np.load(\"/media/hdd01/sklee/CA1_clust8-30/data/clust8_syn_dist.npy\")[:,-1]\n",
    "        C12_dist_raw = np.load(\"/media/hdd01/sklee/CA1_clust12-20/data/clust12_syn_dist.npy\")[:,-1]\n",
    "\n",
    "        C4_diff_scale = np.load(\"/media/hdd01/sklee/CA1_clust4-60/clust/gru_s5_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"E_scale\"]\n",
    "        C4_nona_scale = np.load(\"/media/hdd01/sklee/CA1_clust4-60_noNA/clust/gru_s5_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"E_scale\"]\n",
    "        C8_diff_scale = np.load(\"/media/hdd01/sklee/CA1_clust8-30/clust/gru_s9_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"E_scale\"]\n",
    "        C8_nona_scale = np.load(\"/media/hdd01/sklee/CA1_clust8-30_noNA/clust/gru_s9_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"E_scale\"]\n",
    "        C12_diff_scale = np.load(\"/media/hdd01/sklee/CA1_clust12-20/clust/gru_s13_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"E_scale\"]\n",
    "        C12_nona_scale = np.load(\"/media/hdd01/sklee/CA1_clust12-20_noNA/clust/gru_s13_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"E_scale\"]\n",
    "    \n",
    "    else:\n",
    "        C4_diff_raw = np.load(\"/media/hdd01/sklee/CA1_clust4-60/clust/gru_s5_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"C_syn_e\"]\n",
    "        C4_nona_raw = np.load(\"/media/hdd01/sklee/CA1_clust4-60_noNA/clust/gru_s5_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"C_syn_e\"]\n",
    "        C8_diff_raw = np.load(\"/media/hdd01/sklee/CA1_clust8-30/clust/gru_s9_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"C_syn_e\"]\n",
    "        C8_nona_raw = np.load(\"/media/hdd01/sklee/CA1_clust8-30_noNA/clust/gru_s9_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"C_syn_e\"]\n",
    "        C12_diff_raw = np.load(\"/media/hdd01/sklee/CA1_clust12-20/clust/gru_s13_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"C_syn_e\"]\n",
    "        C12_nona_raw = np.load(\"/media/hdd01/sklee/CA1_clust12-20_noNA/clust/gru_s13_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"C_syn_e\"]\n",
    "\n",
    "        C4_dist_raw = np.load(\"/media/hdd01/sklee/CA1_clust4-60/data/clust4_syn_dist.npy\")[:,-1]\n",
    "        C8_dist_raw = np.load(\"/media/hdd01/sklee/CA1_clust8-30/data/clust8_syn_dist.npy\")[:,-1]\n",
    "        C12_dist_raw = np.load(\"/media/hdd01/sklee/CA1_clust12-20/data/clust12_syn_dist.npy\")[:,-1]\n",
    "\n",
    "        C4_diff_scale = np.load(\"/media/hdd01/sklee/CA1_clust4-60/clust/gru_s5_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"E_scale\"]\n",
    "        C4_nona_scale = np.load(\"/media/hdd01/sklee/CA1_clust4-60_noNA/clust/gru_s5_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"E_scale\"]\n",
    "        C8_diff_scale = np.load(\"/media/hdd01/sklee/CA1_clust8-30/clust/gru_s9_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"E_scale\"]\n",
    "        C8_nona_scale = np.load(\"/media/hdd01/sklee/CA1_clust8-30_noNA/clust/gru_s9_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"E_scale\"]\n",
    "        C12_diff_scale = np.load(\"/media/hdd01/sklee/CA1_clust12-20/clust/gru_s13_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"E_scale\"]\n",
    "        C12_nona_scale = np.load(\"/media/hdd01/sklee/CA1_clust12-20_noNA/clust/gru_s13_h20_pretrain_output_set\"+str(i+1)+\".npz\")[\"E_scale\"]\n",
    "    \n",
    "    C4_diff_scale_all[i] = C4_diff_scale\n",
    "    C4_nona_scale_all[i] = C4_nona_scale\n",
    "    C8_diff_scale_all[i] = C8_diff_scale\n",
    "    C8_nona_scale_all[i] = C8_nona_scale\n",
    "    C12_diff_scale_all[i] = C12_diff_scale\n",
    "    C12_nona_scale_all[i] = C12_nona_scale\n",
    "    \n",
    "    C4_diff_idx = np.zeros((2000))\n",
    "    C4_nona_idx = np.zeros((2000))\n",
    "    C8_diff_idx = np.zeros((2000))\n",
    "    C8_nona_idx = np.zeros((2000))\n",
    "    C12_diff_idx = np.zeros((2000))\n",
    "    C12_nona_idx = np.zeros((2000))\n",
    "    \n",
    "    C4_rand_idx = np.random.randint(0,5,(2000))\n",
    "    C8_rand_idx = np.random.randint(0,8,(2000))\n",
    "    C12_rand_idx = np.random.randint(0,12,(2000))\n",
    "\n",
    "    for j in range(2000):\n",
    "        C4_diff_idx[j] = np.argmax(C4_diff_raw[:,j])\n",
    "        C4_nona_idx[j] = np.argmax(C4_nona_raw[:,j])\n",
    "        C8_diff_idx[j] = np.argmax(C8_diff_raw[:,j])\n",
    "        C8_nona_idx[j] = np.argmax(C8_nona_raw[:,j])\n",
    "        C12_diff_idx[j] = np.argmax(C12_diff_raw[:,j])\n",
    "        C12_nona_idx[j] = np.argmax(C12_nona_raw[:,j])\n",
    "\n",
    "    for j in range(2000):\n",
    "        for k in range(2000):\n",
    "            if C4_diff_idx[j] != C4_diff_idx[k]:\n",
    "                C4_diff_all[i,j,k] = 1\n",
    "                C4_diff_all[i,k,j] = 1\n",
    "            if C4_nona_idx[j] != C4_nona_idx[k]:\n",
    "                C4_nona_all[i,j,k] = 1\n",
    "                C4_nona_all[i,k,j] = 1\n",
    "            if C8_diff_idx[j] != C8_diff_idx[k]:\n",
    "                C8_diff_all[i,j,k] = 1\n",
    "                C8_diff_all[i,k,j] = 1\n",
    "            if C8_nona_idx[j] != C8_nona_idx[k]:\n",
    "                C8_nona_all[i,j,k] = 1\n",
    "                C8_nona_all[i,k,j] = 1\n",
    "            if C12_diff_idx[j] != C12_diff_idx[k]:\n",
    "                C12_diff_all[i,j,k] = 1\n",
    "                C12_diff_all[i,k,j] = 1\n",
    "            if C12_nona_idx[j] != C12_nona_idx[k]:\n",
    "                C12_nona_all[i,j,k] = 1\n",
    "                C12_nona_all[i,k,j] = 1\n",
    "            if C4_rand_idx[j] != C4_rand_idx[k]:\n",
    "                C4_rand_all[i,j,k] = 1\n",
    "                C4_rand_all[i,k,j] = 1\n",
    "            if C8_rand_idx[j] != C8_rand_idx[k]:\n",
    "                C8_rand_all[i,j,k] = 1\n",
    "                C8_rand_all[i,k,j] = 1\n",
    "            if C12_rand_idx[j] != C12_rand_idx[k]:\n",
    "                C12_rand_all[i,j,k] = 1\n",
    "                C12_rand_all[i,k,j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7df60b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29253/2949891472.py:16: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for i in tnrange(2000):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85555b33bd5a4de1a10b03b2f2734584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "C4_clust_idx = np.zeros((2000))\n",
    "C8_clust_idx = np.zeros((2000))\n",
    "C12_clust_idx = np.zeros((2000))\n",
    "\n",
    "for i in range(4):\n",
    "    C4_clust_idx[880+60*i:880+60*(i+1)] = i+1\n",
    "for i in range(8):\n",
    "    C8_clust_idx[880+30*i:880+30*(i+1)] = i+1\n",
    "for i in range(12):\n",
    "    C12_clust_idx[880+20*i:880+20*(i+1)] = i+1\n",
    "    \n",
    "C4_clust = np.zeros((2000,2000))\n",
    "C8_clust = np.zeros((2000,2000))\n",
    "C12_clust = np.zeros((2000,2000))\n",
    "\n",
    "for i in tnrange(2000):\n",
    "    for j in range(2000):\n",
    "        if C4_clust_idx[i] != C4_clust_idx[j]:\n",
    "            C4_clust[i,j] = 1\n",
    "            C4_clust[j,i] = 1\n",
    "        if C8_clust_idx[i] != C8_clust_idx[j]:\n",
    "            C8_clust[i,j] = 1\n",
    "            C8_clust[j,i] = 1\n",
    "        if C12_clust_idx[i] != C12_clust_idx[j]:\n",
    "            C12_clust[i,j] = 1\n",
    "            C12_clust[j,i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef02076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29253/390400844.py:5: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for i in tnrange(2000):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3439b523077f4d81942585bc1f723d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "C4_dist = np.zeros((2000,2000))\n",
    "C8_dist = np.zeros((2000,2000))\n",
    "C12_dist = np.zeros((2000,2000))\n",
    "\n",
    "for i in tnrange(2000):\n",
    "    for j in range(2000):\n",
    "        C4_dist[i,j] = np.abs(C4_dist_raw[i] - C4_dist_raw[j])\n",
    "        C8_dist[i,j] = np.abs(C8_dist_raw[i] - C8_dist_raw[j])\n",
    "        C12_dist[i,j] = np.abs(C12_dist_raw[i] - C12_dist_raw[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d294cfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29253/1194119769.py:21: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for i in tnrange(3):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9197f5f747248838ffcba1bfabbea10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = np.triu_indices(2000, k=0)\n",
    "x_part, y_part = np.triu_indices(240, k=0)\n",
    "x_part += 880\n",
    "y_part += 880\n",
    "\n",
    "within_corrs_weighted = np.zeros((rep_no*(rep_no-1)//2,9))\n",
    "across_corrs_weighted = np.zeros((rep_no**2,3))\n",
    "dist_corrs_weighted = np.zeros((rep_no,9))\n",
    "clust_corrs_weighted = np.zeros((rep_no,9))\n",
    "\n",
    "C4_diff_scale_mat_all = np.matmul(C4_diff_scale_all.reshape(rep_no,-1,1), C4_diff_scale_all.reshape(rep_no,1,-1))\n",
    "C4_nona_scale_mat_all = np.matmul(C4_nona_scale_all.reshape(rep_no,-1,1), C4_nona_scale_all.reshape(rep_no,1,-1))\n",
    "C8_diff_scale_mat_all = np.matmul(C8_diff_scale_all.reshape(rep_no,-1,1), C8_diff_scale_all.reshape(rep_no,1,-1))\n",
    "C8_nona_scale_mat_all = np.matmul(C8_nona_scale_all.reshape(rep_no,-1,1), C8_nona_scale_all.reshape(rep_no,1,-1))\n",
    "C12_diff_scale_mat_all = np.matmul(C12_diff_scale_all.reshape(rep_no,-1,1), C12_diff_scale_all.reshape(rep_no,1,-1))\n",
    "C12_nona_scale_mat_all = np.matmul(C12_nona_scale_all.reshape(rep_no,-1,1), C12_nona_scale_all.reshape(rep_no,1,-1))\n",
    "C4_rand_scale_mat_all = np.matmul(C4_rand_scale_all.reshape(rep_no,-1,1), C4_rand_scale_all.reshape(rep_no,1,-1))\n",
    "C8_rand_scale_mat_all = np.matmul(C8_rand_scale_all.reshape(rep_no,-1,1), C8_rand_scale_all.reshape(rep_no,1,-1))\n",
    "C12_rand_scale_mat_all = np.matmul(C12_rand_scale_all.reshape(rep_no,-1,1), C12_rand_scale_all.reshape(rep_no,1,-1))\n",
    "\n",
    "for i in tnrange(3):\n",
    "    if i == 0:\n",
    "        dist_mat = C4_dist\n",
    "        clust_mat = C4_clust\n",
    "        diff_mat_all = C4_diff_all\n",
    "        nona_mat_all = C4_nona_all\n",
    "        diff_scale_mat_all = C4_diff_scale_mat_all\n",
    "        nona_scale_mat_all = C4_nona_scale_mat_all\n",
    "        rand_mat_all = C4_rand_all\n",
    "        rand_scale_mat_all = C4_rand_scale_mat_all\n",
    "\n",
    "    elif i == 1:\n",
    "        dist_mat = C8_dist\n",
    "        clust_mat = C8_clust\n",
    "        diff_mat_all = C8_diff_all\n",
    "        nona_mat_all = C8_nona_all\n",
    "        diff_scale_mat_all = C8_diff_scale_mat_all\n",
    "        nona_scale_mat_all = C8_nona_scale_mat_all\n",
    "        rand_mat_all = C8_rand_all\n",
    "        rand_scale_mat_all = C8_rand_scale_mat_all\n",
    "\n",
    "    elif i == 2:\n",
    "        dist_mat = C12_dist\n",
    "        clust_mat = C12_clust\n",
    "        diff_mat_all = C12_diff_all\n",
    "        nona_mat_all = C12_nona_all\n",
    "        diff_scale_mat_all = C12_diff_scale_mat_all\n",
    "        nona_scale_mat_all = C12_nona_scale_mat_all\n",
    "        rand_mat_all = C12_rand_all\n",
    "        rand_scale_mat_all = C12_rand_scale_mat_all\n",
    "        \n",
    "    across_count = 0\n",
    "    within_count = 0\n",
    "    for j in range(rep_no):\n",
    "        for k in range(rep_no):\n",
    "            weight = diff_scale_mat_all[j][x,y] * nona_scale_mat_all[k][x,y]\n",
    "            across_corr_val = WeightedCorr(x=pd.Series(diff_mat_all[j][x,y]),\n",
    "                   y=pd.Series(nona_mat_all[k][x,y]),\n",
    "                   w=pd.Series(weight))(method='spearman')\n",
    "            across_corrs_weighted[across_count,i] = across_corr_val\n",
    "            across_count += 1\n",
    "        for k in range(j+1,rep_no,1):\n",
    "            diff_weight = diff_scale_mat_all[j][x,y] * diff_scale_mat_all[k][x,y]\n",
    "            nona_weight = nona_scale_mat_all[j][x,y] * nona_scale_mat_all[k][x,y]\n",
    "            within_diff_corr_val = WeightedCorr(x=pd.Series(diff_mat_all[j][x,y]),\n",
    "                   y=pd.Series(diff_mat_all[k][x,y]),\n",
    "                   w=pd.Series(diff_weight))(method='spearman')\n",
    "            within_nona_corr_val = WeightedCorr(x=pd.Series(nona_mat_all[j][x,y]),\n",
    "                   y=pd.Series(nona_mat_all[k][x,y]),\n",
    "                   w=pd.Series(nona_weight))(method='spearman')\n",
    "            within_corrs_weighted[within_count,i*2] = within_nona_corr_val\n",
    "            within_corrs_weighted[within_count,i*2+1] = within_diff_corr_val\n",
    "            \n",
    "            rand_weight = rand_scale_mat_all[j][x,y] * rand_scale_mat_all[k][x,y]\n",
    "            within_rand_corr_val = WeightedCorr(x=pd.Series(rand_mat_all[j][x,y]),\n",
    "                   y=pd.Series(rand_mat_all[k][x,y]),\n",
    "                   w=pd.Series(rand_weight))(method='spearman')\n",
    "            within_corrs_weighted[within_count,i+6] = within_rand_corr_val            \n",
    "            \n",
    "            within_count += 1\n",
    "        \n",
    "        nona_weight = nona_scale_mat_all[j][x,y]\n",
    "        diff_weight = diff_scale_mat_all[j][x,y]\n",
    "        dist_nona_corr_val = WeightedCorr(x=pd.Series(dist_mat[x,y]),\n",
    "                   y=pd.Series(nona_mat_all[j][x,y]),\n",
    "                   w=pd.Series(nona_weight))(method='spearman')\n",
    "        dist_diff_corr_val = WeightedCorr(x=pd.Series(dist_mat[x,y]),\n",
    "                   y=pd.Series(diff_mat_all[j][x,y]),\n",
    "                   w=pd.Series(diff_weight))(method='spearman')\n",
    "        dist_corrs_weighted[j,i*2] = dist_nona_corr_val\n",
    "        dist_corrs_weighted[j,i*2+1] = dist_diff_corr_val\n",
    "        rand_weight = rand_scale_mat_all[j][x,y]\n",
    "        dist_rand_corr_val = WeightedCorr(x=pd.Series(dist_mat[x,y]),\n",
    "                   y=pd.Series(rand_mat_all[j][x,y]),\n",
    "                   w=pd.Series(rand_weight))(method='spearman')\n",
    "        dist_corrs_weighted[j,i+6] = dist_rand_corr_val\n",
    "        \n",
    "        nona_weight_part = nona_scale_mat_all[j][x_part,y_part]\n",
    "        diff_weight_part = diff_scale_mat_all[j][x_part,y_part]\n",
    "        clust_nona_corr_val = WeightedCorr(x=pd.Series(clust_mat[x_part,y_part]),\n",
    "                   y=pd.Series(nona_mat_all[j][x_part,y_part]),\n",
    "                   w=pd.Series(nona_weight_part))(method='spearman')\n",
    "        clust_diff_corr_val = WeightedCorr(x=pd.Series(clust_mat[x_part,y_part]),\n",
    "                   y=pd.Series(diff_mat_all[j][x_part,y_part]),\n",
    "                   w=pd.Series(diff_weight_part))(method='spearman')\n",
    "        clust_corrs_weighted[j,i*2] = clust_nona_corr_val\n",
    "        clust_corrs_weighted[j,i*2+1] = clust_diff_corr_val\n",
    "        rand_weight_part = rand_scale_mat_all[j][x_part,y_part]\n",
    "        clust_rand_corr_val = WeightedCorr(x=pd.Series(clust_mat[x_part,y_part]),\n",
    "                   y=pd.Series(rand_mat_all[j][x_part,y_part]),\n",
    "                   w=pd.Series(rand_weight_part))(method='spearman')\n",
    "        clust_corrs_weighted[j,i+6] = clust_rand_corr_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92347471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29253/3355600626.py:46: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for i in tnrange(rep_no):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b2ac50e5f648aa97402846858839aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "C4_diff_rep_corr_weighted = np.zeros((rep_no, rep_no))\n",
    "C4_nona_rep_corr_weighted = np.zeros((rep_no, rep_no))\n",
    "C8_diff_rep_corr_weighted = np.zeros((rep_no, rep_no))\n",
    "C8_nona_rep_corr_weighted = np.zeros((rep_no, rep_no))\n",
    "C12_diff_rep_corr_weighted = np.zeros((rep_no, rep_no))\n",
    "C12_nona_rep_corr_weighted = np.zeros((rep_no, rep_no))\n",
    "\n",
    "within_count = 0\n",
    "for i in range(rep_no):\n",
    "    for j in range(i+1,rep_no,1):\n",
    "        C4_nona_rep_corr_weighted[i,j] = within_corrs_weighted[within_count, 0]\n",
    "        C4_nona_rep_corr_weighted[j,i] = within_corrs_weighted[within_count, 0]\n",
    "        C4_diff_rep_corr_weighted[i,j] = within_corrs_weighted[within_count, 1]\n",
    "        C4_diff_rep_corr_weighted[j,i] = within_corrs_weighted[within_count, 1]\n",
    "        C8_nona_rep_corr_weighted[i,j] = within_corrs_weighted[within_count, 2]\n",
    "        C8_nona_rep_corr_weighted[j,i] = within_corrs_weighted[within_count, 2]\n",
    "        C8_diff_rep_corr_weighted[i,j] = within_corrs_weighted[within_count, 3]\n",
    "        C8_diff_rep_corr_weighted[j,i] = within_corrs_weighted[within_count, 3]\n",
    "        C12_nona_rep_corr_weighted[i,j] = within_corrs_weighted[within_count, 4]\n",
    "        C12_nona_rep_corr_weighted[j,i] = within_corrs_weighted[within_count, 4]\n",
    "        C12_diff_rep_corr_weighted[i,j] = within_corrs_weighted[within_count, 5]\n",
    "        C12_diff_rep_corr_weighted[j,i] = within_corrs_weighted[within_count, 5]\n",
    "        \n",
    "        within_count += 1\n",
    "\n",
    "C4_diff_heatmap_weighted = np.zeros((rep_no+3, rep_no))\n",
    "C4_nona_heatmap_weighted = np.zeros((rep_no+3, rep_no))\n",
    "C8_diff_heatmap_weighted = np.zeros((rep_no+3, rep_no))\n",
    "C8_nona_heatmap_weighted = np.zeros((rep_no+3, rep_no))\n",
    "C12_diff_heatmap_weighted = np.zeros((rep_no+3, rep_no))\n",
    "C12_nona_heatmap_weighted = np.zeros((rep_no+3, rep_no))\n",
    "\n",
    "C4_nona_heatmap_weighted[:rep_no,:] = C4_nona_rep_corr_weighted\n",
    "C4_nona_heatmap_weighted[-3,:] = dist_corrs_weighted[:,0]\n",
    "C4_diff_heatmap_weighted[:rep_no,:] = C4_diff_rep_corr_weighted\n",
    "C4_diff_heatmap_weighted[-3,:] = dist_corrs_weighted[:,1]\n",
    "C8_nona_heatmap_weighted[:rep_no,:] = C8_nona_rep_corr_weighted\n",
    "C8_nona_heatmap_weighted[-3,:] = dist_corrs_weighted[:,2]\n",
    "C8_diff_heatmap_weighted[:rep_no,:] = C8_diff_rep_corr_weighted\n",
    "C8_diff_heatmap_weighted[-3,:] = dist_corrs_weighted[:,3]\n",
    "C12_nona_heatmap_weighted[:rep_no,:] = C12_nona_rep_corr_weighted\n",
    "C12_nona_heatmap_weighted[-3,:] = dist_corrs_weighted[:,4]\n",
    "C12_diff_heatmap_weighted[:rep_no,:] = C12_diff_rep_corr_weighted\n",
    "C12_diff_heatmap_weighted[-3,:] = dist_corrs_weighted[:,5]\n",
    "\n",
    "for i in tnrange(rep_no):\n",
    "    C4_nona_rand = WeightedCorr(x=pd.Series(C4_rand_all[i][x,y]),\n",
    "                   y=pd.Series(C4_nona_all[i][x,y]),\n",
    "                   w=pd.Series(C4_nona_scale_mat_all[i][x,y]))(method='spearman')\n",
    "    C4_diff_rand = WeightedCorr(x=pd.Series(C4_rand_all[i][x,y]),\n",
    "                   y=pd.Series(C4_diff_all[i][x,y]),\n",
    "                   w=pd.Series(C4_diff_scale_mat_all[i][x,y]))(method='spearman')\n",
    "    C8_nona_rand = WeightedCorr(x=pd.Series(C8_rand_all[i][x,y]),\n",
    "                   y=pd.Series(C4_nona_all[i][x,y]),\n",
    "                   w=pd.Series(C4_nona_scale_mat_all[i][x,y]))(method='spearman')\n",
    "    C8_diff_rand = WeightedCorr(x=pd.Series(C8_rand_all[i][x,y]),\n",
    "                   y=pd.Series(C4_diff_all[i][x,y]),\n",
    "                   w=pd.Series(C4_diff_scale_mat_all[i][x,y]))(method='spearman')\n",
    "    C12_nona_rand = WeightedCorr(x=pd.Series(C12_rand_all[i][x,y]),\n",
    "                   y=pd.Series(C12_nona_all[i][x,y]),\n",
    "                   w=pd.Series(C12_nona_scale_mat_all[i][x,y]))(method='spearman')\n",
    "    C12_diff_rand = WeightedCorr(x=pd.Series(C12_rand_all[i][x,y]),\n",
    "                   y=pd.Series(C12_diff_all[i][x,y]),\n",
    "                   w=pd.Series(C12_diff_scale_mat_all[i][x,y]))(method='spearman')\n",
    "    \n",
    "    C4_nona_clust = WeightedCorr(x=pd.Series(C4_clust[x_part,y_part]),\n",
    "                   y=pd.Series(C4_nona_all[i][x_part,y_part]),\n",
    "                   w=pd.Series(C4_nona_scale_mat_all[i][x_part,y_part]))(method='spearman')\n",
    "    C4_diff_clust = WeightedCorr(x=pd.Series(C4_clust[x_part,y_part]),\n",
    "                   y=pd.Series(C4_diff_all[i][x_part,y_part]),\n",
    "                   w=pd.Series(C4_diff_scale_mat_all[i][x_part,y_part]))(method='spearman')\n",
    "    C8_nona_clust = WeightedCorr(x=pd.Series(C8_clust[x_part,y_part]),\n",
    "                   y=pd.Series(C4_nona_all[i][x_part,y_part]),\n",
    "                   w=pd.Series(C4_nona_scale_mat_all[i][x_part,y_part]))(method='spearman')\n",
    "    C8_diff_clust = WeightedCorr(x=pd.Series(C8_clust[x_part,y_part]),\n",
    "                   y=pd.Series(C4_diff_all[i][x_part,y_part]),\n",
    "                   w=pd.Series(C4_diff_scale_mat_all[i][x_part,y_part]))(method='spearman')\n",
    "    C12_nona_clust = WeightedCorr(x=pd.Series(C12_clust[x_part,y_part]),\n",
    "                   y=pd.Series(C12_nona_all[i][x_part,y_part]),\n",
    "                   w=pd.Series(C12_nona_scale_mat_all[i][x_part,y_part]))(method='spearman')\n",
    "    C12_diff_clust = WeightedCorr(x=pd.Series(C12_clust[x_part,y_part]),\n",
    "                   y=pd.Series(C12_diff_all[i][x_part,y_part]),\n",
    "                   w=pd.Series(C12_diff_scale_mat_all[i][x_part,y_part]))(method='spearman')\n",
    "    \n",
    "    C4_nona_heatmap_weighted[-1,i] = C4_nona_rand\n",
    "    C4_diff_heatmap_weighted[-1,i] = C4_diff_rand\n",
    "    C8_nona_heatmap_weighted[-1,i] = C8_nona_rand\n",
    "    C8_diff_heatmap_weighted[-1,i] = C8_diff_rand\n",
    "    C12_nona_heatmap_weighted[-1,i] = C12_nona_rand\n",
    "    C12_diff_heatmap_weighted[-1,i] = C12_diff_rand\n",
    "    C4_nona_heatmap_weighted[-2,i] = C4_nona_clust\n",
    "    C4_diff_heatmap_weighted[-2,i] = C4_diff_clust\n",
    "    C8_nona_heatmap_weighted[-2,i] = C8_nona_clust\n",
    "    C8_diff_heatmap_weighted[-2,i] = C8_diff_clust\n",
    "    C12_nona_heatmap_weighted[-2,i] = C12_nona_clust\n",
    "    C12_diff_heatmap_weighted[-2,i] = C12_diff_clust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52931967",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e169d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29253/3833372794.py:182: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  fig.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['font.sans-serif'] = \"Arial\"\n",
    "mpl.rcParams['font.family'] = \"sans-serif\"\n",
    "plt.rcParams['font.size'] = '13'\n",
    "\n",
    "fig = plt.figure(constrained_layout=True, figsize = (14,3))\n",
    "gs = fig.add_gridspec(1,12, wspace=0.1, hspace=0.35)\n",
    "ax1 = fig.add_subplot(gs[0,:3])\n",
    "ax2 = fig.add_subplot(gs[0,3:6])\n",
    "ax3 = fig.add_subplot(gs[0,6:9])\n",
    "ax4 = fig.add_subplot(gs[0,9:])\n",
    "#ax5 = fig.add_subplot(gs[2,:4])\n",
    "#ax6 = fig.add_subplot(gs[2,4:8])\n",
    "#ax7 = fig.add_subplot(gs[2,8:])\n",
    "#ax8 = fig.add_subplot(gs[1,:4])\n",
    "#ax9 = fig.add_subplot(gs[1,4:8])\n",
    "#ax10 = fig.add_subplot(gs[1,8:])\n",
    "\n",
    "colors=[\"blue\", \"red\", \"blue\", \"red\", \"blue\", \"red\"]\n",
    "cap_colors=[\"blue\", \"blue\", \"red\", \"red\",\n",
    "           \"blue\", \"blue\", \"red\", \"red\",\n",
    "           \"blue\", \"blue\", \"red\", \"red\"]\n",
    "bp = ax1.boxplot(within_corrs_weighted[:,:-3], positions=[1,2,4,5,7,8], widths=0.6, patch_artist=True)\n",
    "for i in range(within_corrs_weighted[:,:-3].shape[1]):\n",
    "    plt.setp(bp['boxes'][i], facecolor=colors[i])\n",
    "    plt.setp(bp['boxes'][i], alpha=0.5)\n",
    "    plt.setp(bp[\"fliers\"][i], markeredgecolor=colors[i])\n",
    "    plt.setp(bp[\"boxes\"][i], color=colors[i])\n",
    "    plt.setp(bp[\"medians\"][i], color=colors[i])\n",
    "for i in range(within_corrs_weighted[:,:-3].shape[1]*2):\n",
    "    plt.setp(bp['whiskers'][i], color=cap_colors[i])\n",
    "    plt.setp(bp['caps'][i], color=cap_colors[i])\n",
    "\n",
    "ax1.fill_between(np.arange(0.8,2.3,0.01), np.mean(within_corrs_weighted[:,-3])-np.std(within_corrs_weighted[:,-3]),\n",
    "                 np.mean(within_corrs_weighted[:,-3])+np.std(within_corrs_weighted[:,-3]), color=\"gray\")\n",
    "ax1.fill_between(np.arange(3.8,5.3,0.01), np.mean(within_corrs_weighted[:,-2])-np.std(within_corrs_weighted[:,-2]),\n",
    "                 np.mean(within_corrs_weighted[:,-2])+np.std(within_corrs_weighted[:,-2]), color=\"gray\")\n",
    "ax1.fill_between(np.arange(6.8,8.3,0.01), np.mean(within_corrs_weighted[:,-1])-np.std(within_corrs_weighted[:,-1]),\n",
    "                 np.mean(within_corrs_weighted[:,-1])+np.std(within_corrs_weighted[:,-1]), color=\"gray\")\n",
    "    \n",
    "hB, = ax1.plot(1,1,'blue')\n",
    "hR, = ax1.plot(1,1,'red')\n",
    "ax1.legend((hB, hR),('$\\mathregular{v_{other}}$', '$\\mathregular{v_{Na}}$'), frameon=False, fontsize=11,\n",
    "          loc=(0.5,0.7))\n",
    "hB.set_visible(False)\n",
    "hR.set_visible(False)\n",
    "\n",
    "ax1.set_ylim(-0.2,1)\n",
    "ax1.set_yticks([-0.2,0.2,0.6,1.0])\n",
    "ax1.spines.right.set_visible(False)\n",
    "ax1.spines.top.set_visible(False)\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax1.set_xticks([1.5, 4.5, 7.5])\n",
    "ax1.set_xticklabels([\"4\", \"8\", \"12\"])\n",
    "ax1.tick_params(axis=\"x\", length=0)\n",
    "ax1.set_xlabel(\"no. of clusters\")\n",
    "ax1.set_ylabel(\"Spearman correlation\")\n",
    "ax1.set_title(\"correlation between architectures\\nwithin $\\mathregular{v_{other}}$ and $\\mathregular{v_{Na}}$\")\n",
    "\n",
    "##################################\n",
    "bp = ax2.boxplot(across_corrs_weighted, widths=0.6, patch_artist=True)\n",
    "\n",
    "colors=[\"purple\", \"purple\", \"purple\"]\n",
    "cap_colors=[\"purple\", \"purple\", \"purple\", \"purple\", \"purple\", \"purple\"]\n",
    "for i in range(across_corrs_weighted.shape[1]):\n",
    "    plt.setp(bp['boxes'][i], facecolor=colors[i])\n",
    "    plt.setp(bp['boxes'][i], alpha=0.5)\n",
    "    plt.setp(bp[\"fliers\"][i], markeredgecolor=colors[i])\n",
    "    plt.setp(bp[\"boxes\"][i], color=colors[i])\n",
    "    plt.setp(bp[\"medians\"][i], color=colors[i])\n",
    "for i in range(across_corrs_weighted.shape[1]*2):\n",
    "    plt.setp(bp['whiskers'][i], color=cap_colors[i])\n",
    "    plt.setp(bp['caps'][i], color=cap_colors[i])\n",
    "    \n",
    "ax2.set_ylim(-0.2,1)\n",
    "ax2.set_yticks([-0.2,0.2,0.6,1.0])\n",
    "ax2.spines.right.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax2.spines.bottom.set_visible(False)\n",
    "ax2.set_xticks([1, 2, 3])\n",
    "ax2.set_xticklabels([\"4\", \"8\", \"12\"])\n",
    "ax2.tick_params(axis=\"x\", length=0)\n",
    "ax2.set_xlabel(\"no. of clusters\")\n",
    "ax2.set_title(\"correlation between architectures\\nof $\\mathregular{v_{other}}$ and $\\mathregular{v_{Na}}$\")\n",
    "\n",
    "####################################\n",
    "bp = ax3.boxplot(dist_corrs_weighted[:,:-3], positions=[1,2,4,5,7,8], widths=0.6, patch_artist=True)\n",
    "colors=[\"blue\", \"red\", \"blue\", \"red\", \"blue\", \"red\"]\n",
    "cap_colors=[\"blue\", \"blue\", \"red\", \"red\",\n",
    "           \"blue\", \"blue\", \"red\", \"red\",\n",
    "           \"blue\", \"blue\", \"red\", \"red\"]\n",
    "for i in range(dist_corrs_weighted[:,:-3].shape[1]):\n",
    "    plt.setp(bp['boxes'][i], facecolor=colors[i])\n",
    "    plt.setp(bp['boxes'][i], alpha=0.5)\n",
    "    plt.setp(bp[\"fliers\"][i], markeredgecolor=colors[i])\n",
    "    plt.setp(bp[\"boxes\"][i], color=colors[i])\n",
    "    plt.setp(bp[\"medians\"][i], color=colors[i])\n",
    "for i in range(dist_corrs_weighted[:,:-3].shape[1]*2):\n",
    "    plt.setp(bp['whiskers'][i], color=cap_colors[i])\n",
    "    plt.setp(bp['caps'][i], color=cap_colors[i])\n",
    "ax3.fill_between(np.arange(0.8,2.3,0.01), np.mean(dist_corrs_weighted[:,-3])-np.std(dist_corrs_weighted[:,-3]),\n",
    "                 np.mean(dist_corrs_weighted[:,-3])+np.std(dist_corrs_weighted[:,-3]), color=\"gray\")\n",
    "ax3.fill_between(np.arange(3.8,5.3,0.01), np.mean(dist_corrs_weighted[:,-2])-np.std(dist_corrs_weighted[:,-2]),\n",
    "                 np.mean(dist_corrs_weighted[:,-2])+np.std(dist_corrs_weighted[:,-2]), color=\"gray\")\n",
    "ax3.fill_between(np.arange(6.8,8.3,0.01), np.mean(dist_corrs_weighted[:,-1])-np.std(dist_corrs_weighted[:,-1]),\n",
    "                 np.mean(dist_corrs_weighted[:,-1])+np.std(dist_corrs_weighted[:,-1]), color=\"gray\")\n",
    "\n",
    "ax3.set_ylim(-0.2,1)\n",
    "ax3.set_yticks([-0.2,0.2,0.6,1.0])\n",
    "ax3.spines.right.set_visible(False)\n",
    "ax3.spines.top.set_visible(False)\n",
    "ax3.spines.bottom.set_visible(False)\n",
    "ax3.set_xticks([1.5, 4.5, 7.5])\n",
    "ax3.set_xticklabels([\"4\", \"8\", \"12\"])\n",
    "ax3.tick_params(axis=\"x\", length=0)\n",
    "ax3.set_xlabel(\"no. of clusters\")\n",
    "ax3.set_title(\"correlation between architecture\\nand somatic distance\")\n",
    "\n",
    "####################################\n",
    "bp = ax4.boxplot(clust_corrs_weighted[:,:-3], positions=[1,2,4,5,7,8], widths=0.6, patch_artist=True)\n",
    "colors=[\"blue\", \"red\", \"blue\", \"red\", \"blue\", \"red\"]\n",
    "cap_colors=[\"blue\", \"blue\", \"red\", \"red\",\n",
    "           \"blue\", \"blue\", \"red\", \"red\",\n",
    "           \"blue\", \"blue\", \"red\", \"red\"]\n",
    "for i in range(clust_corrs_weighted[:,:-3].shape[1]):\n",
    "    plt.setp(bp['boxes'][i], facecolor=colors[i])\n",
    "    plt.setp(bp['boxes'][i], alpha=0.5)\n",
    "    plt.setp(bp[\"fliers\"][i], markeredgecolor=colors[i])\n",
    "    plt.setp(bp[\"boxes\"][i], color=colors[i])\n",
    "    plt.setp(bp[\"medians\"][i], color=colors[i])\n",
    "for i in range(clust_corrs_weighted[:,:-3].shape[1]*2):\n",
    "    plt.setp(bp['whiskers'][i], color=cap_colors[i])\n",
    "    plt.setp(bp['caps'][i], color=cap_colors[i])\n",
    "ax4.fill_between(np.arange(0.8,2.3,0.01), np.mean(clust_corrs_weighted[:,-3])-np.std(clust_corrs_weighted[:,-3]),\n",
    "                 np.mean(clust_corrs_weighted[:,-3])+np.std(clust_corrs_weighted[:,-3]), color=\"gray\")\n",
    "ax4.fill_between(np.arange(3.8,5.3,0.01), np.mean(clust_corrs_weighted[:,-2])-np.std(clust_corrs_weighted[:,-2]),\n",
    "                 np.mean(clust_corrs_weighted[:,-2])+np.std(clust_corrs_weighted[:,-2]), color=\"gray\")\n",
    "ax4.fill_between(np.arange(6.8,8.3,0.01), np.mean(clust_corrs_weighted[:,-1])-np.std(clust_corrs_weighted[:,-1]),\n",
    "                 np.mean(clust_corrs_weighted[:,-1])+np.std(clust_corrs_weighted[:,-1]), color=\"gray\")\n",
    "\n",
    "ax4.set_ylim(-0.2,1)\n",
    "ax4.set_yticks([-0.2,0.2,0.6,1.0])\n",
    "ax4.spines.right.set_visible(False)\n",
    "ax4.spines.top.set_visible(False)\n",
    "ax4.spines.bottom.set_visible(False)\n",
    "ax4.set_xticks([1.5, 4.5, 7.5])\n",
    "ax4.set_xticklabels([\"4\", \"8\", \"12\"])\n",
    "ax4.tick_params(axis=\"x\", length=0)\n",
    "ax4.set_xlabel(\"no. of clusters\")\n",
    "ax4.set_title(\"correlation between\\narchitecture and clusters\")\n",
    "\n",
    "\"\"\"\n",
    "######################################\n",
    "im1 = ax5.imshow(C4_diff_heatmap_weighted, origin='lower', vmin=0, vmax=1, cmap=\"jet\")\n",
    "im2 = ax8.imshow(C4_nona_heatmap_weighted, origin='lower', vmin=0, vmax=1, cmap=\"jet\")\n",
    "im3 = ax6.imshow(C8_diff_heatmap_weighted, origin='lower', vmin=0, vmax=1, cmap=\"jet\")\n",
    "im4 = ax9.imshow(C8_nona_heatmap_weighted, origin='lower', vmin=0, vmax=1, cmap=\"jet\")\n",
    "im5 = ax7.imshow(C12_diff_heatmap_weighted, origin='lower', vmin=0, vmax=1, cmap=\"jet\")\n",
    "im6 = ax10.imshow(C12_nona_heatmap_weighted, origin='lower', vmin=0, vmax=1, cmap=\"jet\")\n",
    "\n",
    "plt.colorbar(im1, ax=ax5)\n",
    "plt.colorbar(im2, ax=ax8)\n",
    "plt.colorbar(im3, ax=ax6)\n",
    "plt.colorbar(im4, ax=ax9)\n",
    "plt.colorbar(im5, ax=ax7)\n",
    "plt.colorbar(im6, ax=ax10)\n",
    "\n",
    "ax5.set_title(\"Differential (4 clusters)\", fontsize=12)\n",
    "ax8.set_title(\"No Na+ (4 clusters)\", fontsize=12)\n",
    "ax6.set_title(\"Differential (8 clusters)\", fontsize=12)\n",
    "ax9.set_title(\"No Na+ (8 clusters)\", fontsize=12)\n",
    "ax7.set_title(\"Differential (12 clusters)\", fontsize=12)\n",
    "ax10.set_title(\"No Na+ (12 clusters)\", fontsize=12)\n",
    "\n",
    "for ax in [ax5,ax6,ax7,ax8,ax9,ax10]:\n",
    "    ax.set_xticks(np.arange(5))\n",
    "    ax.set_yticks(np.arange(8))\n",
    "    ax.set_xticklabels([\"1\",\"2\",\"3\",\"4\",\"5\"])\n",
    "    ax.set_yticklabels([\"1\",\"2\",\"3\",\"4\",\"5\",\"Soma Dist.\", \"Clusters\", \"Random\"])\n",
    "\"\"\"\n",
    "\n",
    "fig.tight_layout()\n",
    "#plt.show()\n",
    "#fig.savefig(\"/home/sklee/dendrite/fig9/fig9_raw.pdf\", bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41920e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
