{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vae_hist_glm import VAE_Hist_GLM, NN_Encoder\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tnrange\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/media/hdd01/sklee/\"\n",
    "experiment = \"clust4-60\"\n",
    "cell_type = \"CA1\"\n",
    "\n",
    "E_neural_file = \"Espikes_neural.npy\"\n",
    "I_neural_file = \"Ispikes_neural.npy\"\n",
    "V_file = \"vdata_T10_Ne2000_gA0.6_tauA1_gN0.8_Ni200_gG0.1_gB0.1_Er0.5_Ir7.4_random_NR_rep10_stimseed1.npy\"\n",
    "C_syn_e_file = \"handsub6_C_syn_e.npy\"\n",
    "C_syn_i_file = \"handsub6_C_syn_i.npy\"\n",
    "C_den_file = \"handsub6_C_den.npy\"\n",
    "\n",
    "E_neural = np.load(base_dir+cell_type+\"_\"+experiment+\"/data/\"+E_neural_file)\n",
    "I_neural = np.load(base_dir+cell_type+\"_\"+experiment+\"/data/\"+I_neural_file)\n",
    "V = np.load(base_dir+cell_type+\"_\"+experiment+\"/data/\"+V_file)[:,:50000].flatten()\n",
    "C_syn_e = np.load(base_dir+cell_type+\"_\"+experiment+\"/data/\"+C_syn_e_file)\n",
    "C_syn_i = np.load(base_dir+cell_type+\"_\"+experiment+\"/data/\"+C_syn_i_file)\n",
    "C_den = np.load(base_dir+cell_type+\"_\"+experiment+\"/data/\"+C_den_file)\n",
    "\n",
    "E_neural = torch.from_numpy(E_neural)\n",
    "I_neural = torch.from_numpy(I_neural)\n",
    "C_syn_e = torch.from_numpy(C_syn_e)\n",
    "C_syn_i = torch.from_numpy(C_syn_i)\n",
    "V = torch.from_numpy(V)\n",
    "C_den = torch.from_numpy(C_den)\n",
    "sub_no = C_den.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_train = 60 * 1000 * 5\n",
    "T_test = 10 * 1000 * 5\n",
    "T_no = 500\n",
    "save_dir = base_dir+cell_type+\"_\"+experiment+\"/\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "batch_size = 100000\n",
    "iter_no = 20000\n",
    "epoch_no = 15\n",
    "layer_no = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_train = V[:T_train].to(device).float()\n",
    "V_test = V[T_train:T_train + T_test].to(device).float()\n",
    "test_E_neural = E_neural[T_train:T_train+T_test].float().to(device)\n",
    "test_I_neural = I_neural[T_train:T_train+T_test].float().to(device)\n",
    "train_E_neural = E_neural[:T_train].float().to(device)\n",
    "train_I_neural = I_neural[:T_train].float().to(device)\n",
    "C_syn_e = C_syn_e.float().to(device)\n",
    "C_syn_i = C_syn_i.float().to(device)\n",
    "C_den = C_den.float().to(device)\n",
    "\n",
    "batch_no = (T_train - batch_size) * epoch_no\n",
    "train_idx = np.empty((epoch_no, T_train - batch_size))\n",
    "for i in range(epoch_no):\n",
    "    part_idx = np.arange(T_train - batch_size)\n",
    "    np.random.shuffle(part_idx)\n",
    "    train_idx[i] = part_idx\n",
    "train_idx = train_idx.flatten()\n",
    "train_idx = torch.from_numpy(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15220\n",
      "355\n"
     ]
    }
   ],
   "source": [
    "decoder = VAE_Hist_GLM(C_den, C_syn_e, C_syn_i, T_no, device)\n",
    "encoder = NN_Encoder(C_syn_e, C_syn_i, T_no, layer_no, device)\n",
    "\n",
    "enc_optimizer = torch.optim.Adam(encoder.parameters(), lr = 0.005)\n",
    "dec_optimizer = torch.optim.Adam(decoder.parameters(), lr = 0.001)\n",
    "\n",
    "encoder.to(device).float()\n",
    "decoder.to(device).float()\n",
    "print(sum(p.numel() for p in encoder.parameters() if p.requires_grad))\n",
    "print(sum(p.numel() for p in decoder.parameters() if p.requires_grad))\n",
    "\n",
    "mse_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-aa927502aae0>:1: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for i in tnrange(iter_no):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da839cfe7c94b6c93039e30bc72ddac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 0.00018095970153808594 tensor([-3.9501e-01,  3.1065e-01, -7.7390e-01, -3.8447e-04,  4.2458e-03],\n",
      "       device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "1999 0.00032979249954223633 tensor([-0.2115,  0.1445, -0.1231, -0.0004, -0.0029], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "2999 0.0004658699035644531 tensor([-0.1496,  0.0827,  0.0128, -0.0003, -0.0017], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "3999 0.0003864169120788574 tensor([-8.7255e-02,  3.0979e-02, -3.7906e-03, -2.4602e-04,  6.0895e-05],\n",
      "       device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "4999 0.000156402587890625 tensor([-0.0567,  0.0180,  0.0389, -0.0001, -0.0002], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "5999 9.107589721679688e-05 tensor([-1.9015e-02,  2.8906e-03,  1.1874e-03, -7.5151e-05,  2.1341e-04],\n",
      "       device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "6999 5.0067901611328125e-05 tensor([-0.0047,  0.0005, -0.0029, -0.0002, -0.0005], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-aa927502aae0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mV_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_V\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     V_pred, V_hid_dec, out_filters = decoder.train_forward(batch_E_neural,\n\u001b[0m\u001b[1;32m     18\u001b[0m                                                          \u001b[0mbatch_I_neural\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                                          V_enc)\n",
      "\u001b[0;32m~/dendrite/models/vae_hist_glm.py\u001b[0m in \u001b[0;36mtrain_forward\u001b[0;34m(self, S_e, S_i, V)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mT_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0msyn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspike_convolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mns_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_data\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dendrite/models/vae_hist_glm.py\u001b[0m in \u001b[0;36mspike_convolve\u001b[0;34m(self, S_e, S_i)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0msyn_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC_syn_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mpad_syn_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT_no\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mpad_syn_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT_no\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mpad_syn_e\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mT_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_syn_e\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mT_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msyn_e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tnrange(iter_no):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    enc_optimizer.zero_grad()\n",
    "    dec_optimizer.zero_grad()\n",
    "        \n",
    "    batch_idx = train_idx[i].long()\n",
    "    batch_E_neural = train_E_neural[batch_idx : batch_idx+batch_size]\n",
    "    batch_I_neural = train_I_neural[batch_idx : batch_idx+batch_size]\n",
    "    batch_V = V_train[batch_idx : batch_idx+batch_size]\n",
    "    \n",
    "    V_hid_enc = encoder(batch_V, batch_E_neural, batch_I_neural)\n",
    "    V_enc = torch.zeros(batch_size, sub_no).to(device)\n",
    "    V_enc[:,1:] = V_enc[:,1:] + V_hid_enc\n",
    "    V_enc[:,0] = V_enc[:,0] + batch_V\n",
    "    \n",
    "    V_pred, V_hid_dec, out_filters = decoder.train_forward(batch_E_neural,\n",
    "                                                         batch_I_neural,\n",
    "                                                         V_enc)\n",
    "    \n",
    "    prior_loss = torch.mean(V_hid_enc**2/0.1)\n",
    "    var_loss = torch.var(batch_V - V_pred)\n",
    "    mse_loss = mse_criterion(V_hid_dec, V_hid_enc.detach())\n",
    "    \n",
    "    loss = var_loss + prior_loss + mse_loss\n",
    "    #loss = var_loss + mse_loss\n",
    "    loss.backward()\n",
    "    enc_optimizer.step()\n",
    "    dec_optimizer.step()\n",
    "    \n",
    "    if i%1000 == 999:\n",
    "        V_pred, out_filters = decoder.test_forward(test_E_neural, test_I_neural)\n",
    "        \n",
    "        var_exp = metrics.explained_variance_score(y_true=V_test.cpu().detach().numpy(),\n",
    "                                                      y_pred=V_pred.cpu().detach().numpy())\n",
    "        \n",
    "        print(i, var_exp, torch.mean(V_hid_enc ,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
